{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building complex models we often need to share large sets of variables and we might want to initialize all of them in one place. And this can be done using tf.variable_scope() and tf.get_variable().\n",
    "\n",
    "One way to do variables sharing is to use classes to create a model, where the classes take care of managing the variables they need. For a lighter solution, not involing classes, tensorflow provides a VariableScope mechanism that allows easily share named variables while constructing a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Scope mechanism in tensorflow consists of two main functions like below:\n",
    "* tf.get_variable(<name>, <shape>, <initializer>): creates or returns a variable with a given name;\n",
    "* tf.variable_scope(<scope_name>): manages namespaces for names passed to tf.get_variable().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to understand how tf.get_variable() works.\n",
    "import tensorflow as tf\n",
    "# v = tf.get_variable(name, shape, dtype, initializer)\n",
    "# case1: the scope is set for creating new variables, as evidenced by tf.get_variable_scope().reuse == False\n",
    "# In this case, v will be a newly created tf.Variable. The full name of the created variable will be set to the \n",
    "# current variable scope name + the provided name and a check will be performed to ensure that no variables with\n",
    "# this full name exists yet. if a variable with this name already exists, it will raise a ValueError.\n",
    "#with tf.variable_scope(\"foo\"):\n",
    "#    v = tf.get_variable(\"v\", [1])\n",
    "#assert v.name == \"foo/v:0\"\n",
    "\n",
    "# case2: the scope is set for reusing variables, as evidenced by tf.get_variable_scope().reuse == True\n",
    "# In this case, the call will search for an already existing variable with name equal to 'curren variable scope\n",
    "# name + the provided name. If no such variable exists, a ValueError will be raised. If the variable is found, it\n",
    "# will be returned.\n",
    "with tf.variable_scope(\"foo3\"):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(\"foo3\", reuse = True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v is v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Basics of tf.variable_scope():\n",
    "   The primary function of variable scope is to carry a name that will be used as prefix for variable names and\n",
    "   a reuse-flag to distinguish the two cases described above. Nesting variable scope appends their names in a \n",
    "   way analogous to how directories work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        # The current variable scope can be retrieved using tf.get_variable_scope() and the reuse flag of the\n",
    "        # current variable scope can be set to True by calling 'tf.get_variable_scope().reuse_variables()'\n",
    "        #\n",
    "        # Note: we can't set the reuse flag to False. The reason behind this is to allow to compose functions that\n",
    "        # create models.\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        v1 = tf.get_variable(\"v\", [1])\n",
    "assert v.name == \"foo/bar/v:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"root\"):\n",
    "    # At start, the scope is not reusing.\n",
    "    assert tf.get_variable_scope().reuse == False\n",
    "    with tf.variable_scope(\"foo\"):\n",
    "        # Opened a sub-scope, still not reusing.\n",
    "        assert tf.get_variable_scope().reuse == False\n",
    "    with tf.variable_scope(\"foo\", reuse=True):\n",
    "        # Explicitly opened a reusing scope.\n",
    "        assert tf.get_variable_scope().reuse == True\n",
    "        with tf.variable_scope(\"bar\"):\n",
    "            # Now sub-scope inherits the reuse flag.\n",
    "            assert tf.get_variable_scope().reuse == True\n",
    "    # Exited the reusing block, back to a non-reusing one.\n",
    "    assert tf.get_variable_scope().reuse == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can share parameters only by using variable with their names agreed, that is because we opened a reusing\n",
    "# variable scope with exactly the same string. In more complex cases, it is useful to pass a VariableScope\n",
    "# object rather than rely on getting the names right.\n",
    "with tf.variable_scope(\"foo1\") as foo_scope:\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(foo_scope):\n",
    "    w = tf.get_variable(\"w\", [1])\n",
    "with tf.variable_scope(foo_scope, reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "    w1 = tf.get_variable(\"w\", [1])\n",
    "assert v1 is v and w1 is w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When opening a variable scope using a previously existing scope we jump out of the current variable scope\n",
    "# prefix to an entirely different one. This is fully independent of where we do it.\n",
    "with tf.variable_scope(\"foo2\") as foo2_scope:\n",
    "    assert foo2_scope.name == \"foo2\"\n",
    "with tf.variable_scope(\"bar\"):\n",
    "    with tf.variable_scope(\"baz\") as other_scope:\n",
    "        assert other_scope.name == \"bar/baz\"\n",
    "        with tf.variable_scope(foo2_scope):\n",
    "            assert foo2_scope.name == \"foo2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable scope can carry a default initializer. It's inherited by sub-scopes and passed to each tf.get_variable()\n",
    "# call. But it will be overridden if another initializer is specified explicitly.\n",
    "sess = tf.Session()\n",
    "with tf.variable_scope(\"foo9\", initializer = tf.constant_initializer(0.4)):\n",
    "    #v = tf.get_variable(\"v\", [1])\n",
    "    #print v\n",
    "    #print sess.run(v)\n",
    "    #assert v.eval(session=sess) == 0.4 #Default initializer as set above\n",
    "    w = tf.get_variable(\"w\", [1], initializer = tf.constant_initializer(0.3))\n",
    "    #assert w.eval(session=sess) == 0.3 #Specific initializer overrides the default\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        #assert v.eval(session=sess) == 0.4    #Inherited default initializer\n",
    "    with tf.variable_scope(\"baz\", initializer = tf.constant_initializer(0.2)):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        #assert v.eval(session=sess) == 0.2 # Changed default initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boo1/add\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-bd41cba33edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"boo1/v:0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"boo1/bar/add\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf.variable_scope also influences the names of other ops in the scope. When we do with tf.variable_scope(\"name\")\n",
    "# , this implicitly opens a tf.name_scope(\"name\"). like below:\n",
    "with tf.variable_scope(\"boo1\"):\n",
    "    x = 1.0 + tf.get_variable(\"v\", [1])\n",
    "print x.op.name\n",
    "assert x.op.name == \"boo1/add\"\n",
    "\n",
    "# Name scopes can be opened in addition to a variable scope, and they will only affect the names of the ops, but\n",
    "# not of variables.\n",
    "with tf.variable_scope(\"boo1\", reuse=True):\n",
    "    with tf.name_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        x = 1.0 + v\n",
    "assert v.name == \"boo1/v:0\"\n",
    "assert x.op.name == \"boo1/bar/add\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When opening a variable scope using a captured object instead of a string, we do not alter the current name scope for ops.\n",
    "In particular, variable scope is heavily used for recurrent neural networks and sequence-to-sequence models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
