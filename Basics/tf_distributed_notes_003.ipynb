{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow provides a high performance models using benchmark and it utilizes some flexible distributed training mechanism like 'parameter server', 'replicated' and 'cross replica'. I'll give a deep dive onto its insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They provide a class called 'BenchmarkCNN' to do the whole benchmarking work. We'll analyze it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BenchmarkCNN(object):\n",
    "    '''Class for benchmarking a cnn network'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Will initialize lots of parameters used to do model training'''\n",
    "        self.model\n",
    "        self.model_conf\n",
    "        self.trace_filename\n",
    "        self.data_format\n",
    "        self.num_batches\n",
    "        self.num_warmup_batches\n",
    "        self.graph_file\n",
    "        self.resize_method\n",
    "        self.sync_queue_counter\n",
    "        self.num_gpus\n",
    "        self.batch_size = self.model_conf.get_batch_size() * self.num_gpus\n",
    "        assert self.model_conf.get_learning_rate() > 0.0\n",
    "        \n",
    "        #distributed training parameters\n",
    "        self.job_name\n",
    "        self.ps_hosts\n",
    "        self.worker_hosts\n",
    "        self.dataset\n",
    "        self.data_name\n",
    "        self.local_parameter_device_flag\n",
    "        \n",
    "        \n",
    "        #Now based on if we're doing multi-nodes training or single node training, start different path\n",
    "        if self.job_name:\n",
    "            self.task_index\n",
    "            self.cluster = tf.train.ClusterSpec({'ps': self.ps_hosts, 'worker': self.worker_hosts})\n",
    "            self.server = tf.train.Server(self.cluster, job_name = self.job_name, \n",
    "                                         task_index = self.task_index,\n",
    "                                         config = create_config_proto(),\n",
    "                                         protocol = FLAGS.server_protocol)\n",
    "            worker_prefix = '/job:worker/task:%s'% self.task_index\n",
    "            self.param_server_device = tf.train.replica_device_setter(\n",
    "                    worker_device = worker_prefix + '/cpu:0', cluster = self.cluster)\n",
    "            #This device on which the queues for managing synchronization between servers should be stored.\n",
    "            num_ps = len(self.ps_hosts)\n",
    "            self.sync_queue_devices = ['/job:ps/task:%s/cpu:0' % i \n",
    "                                          for i in range(num_ps)]\n",
    "        else:\n",
    "            self.task_index = 0\n",
    "            self.cluster = None\n",
    "            self.server = None\n",
    "            worker_prefix = ''\n",
    "            self.param_server_device = '/%s:0' % FLAGS.local_parameter_device\n",
    "            self.sync_queue_devices = [self.param_server_device]\n",
    "            \n",
    "        \n",
    "        #Device to use for ops that need to always run on the local worker's CPU\n",
    "        self.cpu_device = '%s/cpu:0' % worker_prefix\n",
    "        \n",
    "        #Device to use for ops that need to always run on the local worker's compute device (like GPU/TPU),\n",
    "        #and never on a parameter service device.\n",
    "        self.raw_devices = ['%s/%s:%i' % (worker_prefix, FLAGS.device, i)\n",
    "                           for i in xrange(FLAGS.num_gpus)]\n",
    "        \n",
    "        #Right now TF only supports staged_vars used on 'parameter server' variable update mechanism\n",
    "        #Attach different 'variable manager' object based on different 'variable_update' and 'staged_vars' settings\n",
    "        if FLAGS.variable_update == 'parameter_server':\n",
    "            if self.job_name:\n",
    "                if not FLAGS.staged_vars:\n",
    "                    self.variable_mgr = variable_mgr.VariableMgrDistributedFetchFromPS(self)\n",
    "                else:\n",
    "                    .... = variable_mgr.VariableMgrDistributedFetchFromStagedPS(self)\n",
    "            else:\n",
    "                if not FLAGS.staged_vars:\n",
    "                    .... = variable_mgr.VariableMgrLocalFetchFromPS(self)\n",
    "                else:\n",
    "                    .... = .....VariableMgrLocalFetchFromStagedPS(self)\n",
    "        elif ....variable_update == 'replicated':\n",
    "            if self.job_name:\n",
    "                raise ValueError('Invalid--')\n",
    "            self.variable_mgr = variable_mgr.VariableMgrLocalReplicated(self, FLAGS.use_nccl)\n",
    "        elif ..... == 'distributed_replicated':\n",
    "            if not self.job_name:\n",
    "                raise ValueError('Invalid--')\n",
    "            self.variable_mgr = variable_mgr.VariableMgrDistributedReplicated(self)\n",
    "        elif ..... == 'independent':\n",
    "            if self.job_name:\n",
    "                raise ValueError('Invalid--')\n",
    "            self.variable_mgr = variable_mgr.VariableMgrIndependent(self)\n",
    "        else:\n",
    "            raise ValueError('Invalid--')\n",
    "            \n",
    "        \n",
    "        #Device to use for running on the local worker's compute device, but with variables assigned to\n",
    "        #parameter server devices.\n",
    "        self.devices = self.variable_mgr.get_devices()\n",
    "        \n",
    "        #Control global step \n",
    "        if self.job_name:\n",
    "            self.global_step_device = self.param_server_device\n",
    "        else:\n",
    "            self.global_step_device = self.cpu_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(self):\n",
    "    '''Start the benchmark based on some flags variables'''\n",
    "    if FLAGS.job_name == 'ps':\n",
    "        self.server.join()\n",
    "        return\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        if FLAGS.eval:\n",
    "            self._eval_cnn()\n",
    "        else:\n",
    "            self._benchmark_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _eval_cnn(self):\n",
    "    '''Evaluate the model from a checkpoint using validation dataset.'''\n",
    "    #build model by using inner method\n",
    "    (enqueue_ops, fetches) = self._build_model()\n",
    "    #use saver to load or save checkpoints\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    #use summary to keep event logs to further analyze the data\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, tf.get_default_graph())\n",
    "    \n",
    "    target = ''\n",
    "    # Session is a class for running tensorflow operations; a 'Session' object encapsulates the environment in which\n",
    "    # 'Operation' objects are executed, and 'Tensor' objects are evaluated.\n",
    "    # A session may own resources, such as @{tf.Variable}, @{tf.QueueBase}, and @{tf.ReaderBase}. It's important to\n",
    "    # release these resources when they're no longer required. To do this, either invoke the @{tf.Session.close} \n",
    "    # method on the session, or use the session as a context manager.\n",
    "    # The ['ConfigProto'] protocol buffer exposes various configuration options for a session.\n",
    "    # Its __init__ method interface can be seen below:\n",
    "    # def __init__(self, target = '',graph = None, config = None):\n",
    "    # if no 'graph' argument is specified when constructing the session, the default graph will belaunched in the \n",
    "    # session. If using more than one graph in the same process,you'll have to use different sessions for each graph,\n",
    "    # but each graph can be used in multiple sessions.\n",
    "    # target: The execution engine to connect to. defaults to using an in-process engine. also can be distributed\n",
    "    with tf.Session(target = target, config = create_config_proto()) as sess:\n",
    "        for in in xrange(len(enqueue_ops)):\n",
    "            sess.run(enqueue_ops[:(i + 1)])\n",
    "        global_step = load_checkpoint(saver, sess, FLAGS.train_dir)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        count_top_1 = 0.0\n",
    "        count_top_5 = 0.0\n",
    "        total_eval_count = self.num_batches * self.batch_size\n",
    "        for step in xrange(self.num_batches):\n",
    "            results = sess.run(fetches)\n",
    "            count_top_1 += results[0]\n",
    "            count_top_5 += results[1]\n",
    "            if (step + 1) % FLAGS.display_every == 0:\n",
    "                duration = time.time() = start_time\n",
    "                examples_per_sec = self.batch_size * FLAGS.display_every / duration\n",
    "                start_time = time.time()\n",
    "        precision_at_1 = count_top_1 / total_eval_count\n",
    "        recall_at_5 = count_top_5 / total_eval_count\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag = 'eval/Accuracy@1', simple_value = precision_at_1)\n",
    "        summary.value.add(tag = 'eval/Recall@5', simple_value = recall_at_5)\n",
    "        summary_writer.add_summary(summary, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _benchmark_cnn(self):\n",
    "    '''Run cnn in benchmark mode. When forward_only on, it forwards CNN.'''\n",
    "    # Within this function,we can see how various computing agents sync with each other\n",
    "    (enqueue_ops, fetches) = self._build_model() #build the graph to run\n",
    "    main_fetch_group = tf.group(*fetches)\n",
    "    execution_barrier = None # only used on distributed training mode\n",
    "    # so while do cluster training, cross_replica_sync (false means 'parameter server' mode and true means distributed 'replicated' mode.)\n",
    "    if self.job_name and not FLAGS.cross_replica_sync:\n",
    "        execution_barrier = self.add_sync_queues_and_barrier('execution_barrier_', [])\n",
    "    \n",
    "    # Get the global step tensor.\n",
    "    # The global step tensor must be an integer variable. We first try to find it in the collection 'GLOBAL_STEP',\n",
    "    # or by name 'global_step: 0'.\n",
    "    global_step = tf.contrib.framework.get_global_step() \n",
    "    with tf.device(self.global_step_device):\n",
    "        with tf.control_dependencies([main_fetch_group]):\n",
    "            inc_global_step = global_step.assign_add(1)\n",
    "            fetches.append(inc_global_step)\n",
    "    \n",
    "    if self.job_name and FLAGS.cross_replica_sync:\n",
    "        # Block all replicas until all replicas are ready for next step.\n",
    "        # Used on 'distributed replica' training mode\n",
    "        fetches.append(self.add_sync_queues_and_barrier('sync_queues_step_end_', [main_fetch_group]))\n",
    "    \n",
    "    \n",
    "    variable_mgr_post_init_ops = self.variable_mgr.get_post_init_ops()\n",
    "    if variable_mgr_post_init_ops:\n",
    "        post_init_op_group = tf.group(*variable_mgr_post_init_ops)\n",
    "    else:\n",
    "        post_init_op_group = None\n",
    "    \n",
    "    local_var_init_op = tf.local_variables_initializer()\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    is_chief = (not self.job_name or self.task_index == 0) # If distributed mode, only record summary on 'master' node\n",
    "    summary_writer = None\n",
    "    if (is_chief and FLAGS.summary_verbosity and FLAGS.train_dir and FLAGS.save_summaries_steps > 0):\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.train_dir, tf.get_default_graph())\n",
    "        \n",
    "    # Run the summaries in the same thread as the training operations by passing in None for summary_op to avoid a\n",
    "    # summary_thread being started.\n",
    "    # Running summaries and training operations in parallel could run out of GPU memory.\n",
    "    # However kept to have individual thread here for saving 'checkpoints' to designate our 'saver'\n",
    "    sv = tf.train.Supervisor(\n",
    "        is_chief = is_chief,\n",
    "        logdir = FLAGS.train_dir,\n",
    "        saver = tf.train.Saver(tf.global_variables()),\n",
    "        global_step = global_step,\n",
    "        summary_op = None,\n",
    "        save_model_secs = FLAGS.save_model_secs,\n",
    "        summary_writer = summary_writer)\n",
    "    \n",
    "    step_train_times = []\n",
    "    with sv.managed_session(\n",
    "        master = self.server.target if self.server else '',\n",
    "        config = create_config_proto(),\n",
    "        start_standard_services = FLAGS.summary_verbosity > 0) as sess:\n",
    "        for i in xrange(len(enqueue_ops)):\n",
    "            sess.run(enqueue_ops[:(i + 1)])\n",
    "        sess.run(local_var_init_op)\n",
    "        if post_init_op_group:\n",
    "            sess.run(post_init_op_group)\n",
    "        \n",
    "        init_global_step = 0\n",
    "        if FLAGS.pretrain_dir is not None:\n",
    "            init_global_step = load_checkpoint(sv.saver, sess, FLAGS.pretrain_dir)\n",
    "        global_step_watcher = GlobalStepWatcher(sess, global_step,\n",
    "                                               len(self.worker_hosts) * self.num_warmup_batches + init_global_step,\n",
    "                                               len(self.worker_hosts) * (self.num_warmup_batches + self.num_batches) - 1)\n",
    "        global_step_watcher.start()\n",
    "        \n",
    "        if self.graph_file is not None:\n",
    "            path, filename = os.path.split(self.graph_file)\n",
    "            as_text = filename.endswith('txt')\n",
    "            tf.train.write_graph(sess.graph_def, path, filename, as_text)\n",
    "            \n",
    "        local_step = -1 * self.num_warmup_batches\n",
    "        \n",
    "        if FLAGS.cross_replica_sync and FLAGS.job_name:\n",
    "            # In cross-replica sync mode, all workers must run the same number of local steps, or else the workers \n",
    "            # running the extra step will block.\n",
    "            done_fn = lambda: local_step == self.num_batches\n",
    "        else:\n",
    "            done_fn = lambda: global_step_watcher.done()\n",
    "        while not done_fn():\n",
    "            if local_step == 0:\n",
    "                #Done warm up\n",
    "                if execution_barrier:\n",
    "                    #Wait for other replicas to finish warm up\n",
    "                    assert global_step_watcher.start_time == 0\n",
    "                    sess.run([execution_barrier])\n",
    "                \n",
    "                assert len(step_train_times) == self.num_warmup_batches\n",
    "                step_train_times = summary_op\n",
    "            \n",
    "            if (summary_writer and (local_step + 1) % FLAGS.save_summaries_steps == 0):\n",
    "                fetch_summary = summary_op\n",
    "            else:\n",
    "                fetch_summary = None\n",
    "            summary_str = benchmark_one_step(sess, fetches, local_step, self.batch_size, step_train_times,\n",
    "                                             self.trace_filename, fetch_summary)\n",
    "            if summary_str is not None and is_chief:\n",
    "                sv.summary_computed(sess, summary_str)\n",
    "            local_step += 1\n",
    "        \n",
    "        #waits for the global step to be done, regardless of done_fn.\n",
    "        while not global_step_watcher.done():\n",
    "            time.sleep(.25)\n",
    "        log_fn('total images/sec: %.2f' % (global_step_watcher.steps_per_second() * self.batch_size))\n",
    "        \n",
    "        #Save the model checkpoint\n",
    "        if FLAGS.train_dir is not None and is_chief:\n",
    "            checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n",
    "            if not gfile.Exists(FLAGS.train_dir):\n",
    "                gfile.MakeDirs(FLAGS.train_dir)\n",
    "            sv.saver.save(sess, checkpoint_path, global_step)\n",
    "        \n",
    "        if execution_barrier:\n",
    "            # Wait for other workers to reach the end, so this worker doesn't go away underneath them.\n",
    "            sess.run([execution_barrier])\n",
    "    \n",
    "    sv.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_model(self):\n",
    "    '''Build the Tensorflow graph.'''\n",
    "    image_size = self.model_conf.get_image_size()\n",
    "    data_type = tf.float32\n",
    "    input_data_type = tf.float32\n",
    "    input_nchan = 3\n",
    "    # Sets the graph-level random seed.\n",
    "    # Operations that rely on a random seed actually derive it from two seeds: the graph-level and operation-level\n",
    "    # seeds. This sets the graph-level seed.\n",
    "    # If either graph-level seed or op-level seed set, op will choose it as the random seed. If both of them are set,\n",
    "    # then both seeds are used in conjunction to determine the random sequence.\n",
    "    # To generate different sequences accross sessions, set neither graph-level nor op-level seeds.\n",
    "    tf.set_random_seed(1234)\n",
    "    np.random.seed(4321)\n",
    "    phase_train = not (FLAGS.eval or FLAGS.forward_only)\n",
    "    \n",
    "    losses = []\n",
    "    device_grads = []\n",
    "    all_logits = []\n",
    "    all_top_1_ops = []\n",
    "    all_top_5_ops = []\n",
    "    # Below lists are used to do pipeline processing which is a classical concurrent model\n",
    "    enqueue_ops = []\n",
    "    gpu_copy_stage_ops = []\n",
    "    gpu_compute_stage_ops = []\n",
    "    gpu_grad_stage_ops = []\n",
    "    \n",
    "    use_synthetic_gpu_images = (self.dataset is None)\n",
    "    \n",
    "    with tf.device(self.global_step_device):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        \n",
    "    # Build the processing and model for the worker.\n",
    "    with tf.device(self.cpu_device):\n",
    "        nclass, images_splits, labels_splits = add_image_preprocessing(\n",
    "                        self.dataset, input_nchan, image_size, self.batch_size,\n",
    "                        len(self.devices), input_date_type, self.resize_method, not FLAGS.eval)\n",
    "    \n",
    "    update_ops = None\n",
    "    staging_delta_ops = []\n",
    "    \n",
    "    for device_num in xrange(len(self.devices)):\n",
    "        # Below create the tf.variable_scope around all model graph operations.\n",
    "        with self.variable_mgr.create_outer_variable_scope(device_num), tf.name_scope('tower_%i' % device_num) as \n",
    "        name_scope:\n",
    "            results = self.add_forward_pass_and_gradients(\n",
    "                images_splits[device_num], labels_splits[device_num], nclass, phase_train, device_num,\n",
    "                input_data_type, data_type, input_nchan, use_synthetic_gpu_images, gpu_copy_stage_ops,\n",
    "                gpu_compute_stage_ops, gpu_grad_stage_ops)\n",
    "            if phase_train:\n",
    "                losses.append(results[0])\n",
    "                device_grads.append(results[1])\n",
    "            else:\n",
    "                all_logits.append(results[0])\n",
    "                all_top_1_ops.append(results[1])\n",
    "                all_top_5_ops.append(results[2])\n",
    "            \n",
    "            # Return if only updates for the first GPU tower should be applied.\n",
    "            # device_num == 0 and not self.each_tower_has_variables()\n",
    "            if self.variable_mgr.retain_tower_updates(device_num):\n",
    "                #Retain the Batch Normalization updates operations only from the first tower. Ideally, we should \n",
    "                #grab the updates from all towers but these stats accumulate extremely fast so we can ignore the other\n",
    "                #stats from the other towers without significant detriment.\n",
    "                # key to collect 'update_ops'\n",
    "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scope)\n",
    "                staging_delta_ops = list(self.variable_mgr.staging_delta_ops)\n",
    "    \n",
    "    if not update_ops:\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scope)\n",
    "    enqueue_ops.append(tf.group(*gpu_copy_stage_ops))\n",
    "    if self.variable_mgr.supports_staged_vars():\n",
    "        for staging_ops in self.variable_mgr.staging_vars_on_devices:\n",
    "            gpu_compute_stage_ops.extend(\n",
    "                [put_op for _, (put_op, _) in six.iteritems(staging_ops)])\n",
    "    enqueue_ops.append(tf.group(*gpu_compute_stage_ops))\n",
    "    if gpu_grad_stage_ops:\n",
    "        staging_delta_ops += gpu_grad_stage_ops\n",
    "    if staging_delta_ops:\n",
    "        enqueue_ops.append(tf.group(*(staging_delta_ops)))\n",
    "    \n",
    "    if not phase_train:\n",
    "        if FLAGS.forward_only:\n",
    "            all_logits = tf.concat(all_logits, 0)\n",
    "            fetches = [all_logits] + enqueue_ops\n",
    "        else:\n",
    "            all_top_1_ops = tf.reduce_sum(all_top_1_ops)\n",
    "            all_top_5_ops = tf.reduce_sum(all_top_5_ops)\n",
    "            fetches = [all_top_1_ops, all_top_5_ops] + enqueue_ops\n",
    "        return (enqueue_ops, fetches)\n",
    "    \n",
    "    extra_nccl_ops = []\n",
    "    # Preprocess the device gradients prior to applying them\n",
    "    apply_gradient_devices, gradient_state = (\n",
    "        self.variable_mgr.preprocess_device_grads(device_grads))\n",
    "    \n",
    "    training_ops = []\n",
    "    for d, device in enumerate(apply_gradient_devices):\n",
    "        total_loss = tf.reduce_mean(losses)\n",
    "        # Obtains the [(gradient, variable)] to apply for device_num (d here)\n",
    "        avg_grads = self.variable_mgr.get_gradients_to_apply(d, gradient_state)\n",
    "        \n",
    "        gradient_clip = FLAGS.gradient_clip\n",
    "        learning_rate = self.model_conf.get_learning_rate()\n",
    "        if self.dataset and FLAGS.num_epochs_per_decay > 0:\n",
    "            num_batches_per_epoch = (\n",
    "                self.dataset.num_examples_per_epoch() / self.batch_size)\n",
    "            decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay)\n",
    "            \n",
    "            # Decay the learning rate exponentially based on the number of steps.\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                FLAGS.learning_rate, global_step,\n",
    "                decay_steps, FLAGS.learning_rate_decay_factor, staircase = True)\n",
    "        \n",
    "        if gradient_clip is not None:\n",
    "            clipped_grads = [\n",
    "                (tf.clip_by_value(grad, -gradient_clip, +gradient_clip), var)\n",
    "                for grad, var in avg_grads\n",
    "            ]\n",
    "        else:\n",
    "            clipped_grads = avg_grads\n",
    "        \n",
    "        # Then choose which optimizer type we're gonna to use\n",
    "        if FLAGS.optimizer == 'momentum':\n",
    "            opt = tf.train.MomentumOptimizer(\n",
    "                learning_rate, FLAGS.momentum, use_nesterov = True)\n",
    "        elif FLAGS.optimizer == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        elif FLAGS.optimizer == 'rmsprop':\n",
    "            opt = tf.train.RMSPropOptimizer(learning_rate, FLAGS.rmsprop_decay, \n",
    "                                           momentum = FLAGS.rmsprop_momentum,\n",
    "                                           epsilon = FLAGS.rmsprop_epsilon)\n",
    "        else:\n",
    "            raise ValueError('Optimizer not found')\n",
    "        \n",
    "        # Adds training ops for grads to 'training_ops'\n",
    "        self.variable_mgr.append_apply_gradients_ops(\n",
    "            gradient_state, opt, clipped_grads, training_ops)\n",
    "        \n",
    "        train_op = tf.group(*(train_ops + update_ops + extra_nccl_ops))\n",
    "        \n",
    "        with tf.device(self.cpu_device):\n",
    "            if self.task_index == 0 and FLAGS.summary_verbosity > 0:\n",
    "                tf.summary.scalar('learning_rate', learning_rate)\n",
    "                tf.summary.scalar('total_loss', total_loss)\n",
    "                for grad, var in avg_grads:\n",
    "                    if grad is not None:\n",
    "                        tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "                for var in tf.trainable_variables():\n",
    "                    tf.summary.histogram(var.op.name, var)\n",
    "        fetches = [train_op, total_loss] + enqueue_ops\n",
    "        return (enqueue_ops, fetches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_forward_pass_and_gradients(\n",
    "    self, host_images, host_labels, nclass, phase_train, device_num,\n",
    "    input_data_type, data_type, input_nchan, use_synthetic_gpu_images,\n",
    "    gpu_copy_stage_ops, gpu_compute_stage_ops, gpu_grad_stage_ops):\n",
    "    '''Add ops for forward-pass and gradient comutations.'''\n",
    "    if not use_synthetic_gpu_images:\n",
    "        with tf.device(self.cpu_device):\n",
    "            images_shape = host_images.get_shape()\n",
    "            labels_shape = host_labels.get_shape()\n",
    "            gpu_copy_stage = data_flow_ops.StagingArea(\n",
    "                [tf.float32, tf.int32],\n",
    "                shapes = [images_shape, labels_shape])\n",
    "            gpu_copy_stage_op = gpu_copy_stage.put(\n",
    "                [host_images, host_labels])\n",
    "            gpu_copy_stage_ops.append(gpu_copy_stage_op)\n",
    "            host_images, host_labels = gpu_copy_stage.get()\n",
    "    \n",
    "    with tf.device(self.raw_devices[device_num]):\n",
    "        if not use_synthetic_gpu_images:\n",
    "            gpu_compute_stage = data_flow_ops.StagingArea(\n",
    "                [tf.float32, tf.int32],\n",
    "                shapes = [images_shape, labels_shape]\n",
    "                )\n",
    "            # The CPU-to-GPU copy is triggered here\n",
    "            gpu_compute_stage_op = gpu_compute_stage.put(\n",
    "                [host_images, host_labels])\n",
    "            images, labels = gpu_compute_stage.get()\n",
    "            images = tf.reshape(images, shape = images_shape)\n",
    "            gpu_compute_stage_ops.append(gpu_compute_stage_op)\n",
    "        else:\n",
    "            # Minor hack to avoid H2D copy when using synthetic data\n",
    "            images = tf.truncated_normal(\n",
    "                host_images.get_shape(),\n",
    "                dtype = input_data_type,\n",
    "                stddev = 1e-1,\n",
    "                name = 'synthetic_images')\n",
    "            images = tf.contrib.framework.local_variable(\n",
    "                images, name = 'gpu_cached_images')\n",
    "            labels = host_labels\n",
    "    \n",
    "    with tf.device(self.devices[device_num]):\n",
    "        # Rescale to [0,1)\n",
    "        images *= 1. / 256\n",
    "        # Rescale to [-1, 1] instead of [0, 1)\n",
    "        images = tf.subtract(images, 0.5)\n",
    "        images = tf.multiply(images, 2.0)\n",
    "        \n",
    "        if self.data_format == 'NCHW':\n",
    "            images = tf.transpose(images, [0, 3, 1, 2])\n",
    "        if input_data_type != data_type:\n",
    "            images = tf.cast(images, data_type)\n",
    "        network = ConvNetBuilder(\n",
    "            images, input_nchan, phase_train, self.data_format, data_type)\n",
    "        self.model_conf.add_inference(network)\n",
    "        # Add the final fully-connected class layer\n",
    "        logits = network.affine(nclass, activation = 'linear')\n",
    "        if not phase_train:\n",
    "            top_1_op = tf.reduce_sum(\n",
    "                tf.cast(tf.nn.in_top_k(logits, labels, 1), data_type))\n",
    "            top_5_op = tf.reduce_sum(\n",
    "                tf.cast(tf.nn.in_top_k(logits, labels, 5), data_type))\n",
    "            return (logits, top_2_op, top_5_op)\n",
    "        loss = loss_function(logits, labels)\n",
    "        params = self.variable_mgr.trainable_variables_on_device(device_num)\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in params])\n",
    "        weight_decay = FLAGS.weight_decay\n",
    "        if weight_decay is not None and weight_decay != 0:\n",
    "            loss += weight_decay * l2_loss\n",
    "        \n",
    "        aggmeth = tf.AggregationMethod.DEFAULT\n",
    "        grads = tf.gradients(loss, params, aggregation_method = aggmeth)\n",
    "        \n",
    "        if FLAGS.staged_vars:\n",
    "            grad_dtypes = [grad.dtype for grad in grads]\n",
    "            grad_shapes = [grad.shape for grad in grads]\n",
    "            grad_stage = data_flow_ops.StagingArea(grad_dtypes, grad_shapes)\n",
    "            grad_stage_op = grad_stage.put(grads)\n",
    "            # In general, this decouples the computation of the gradients and the updates of the weights.\n",
    "            # During  the pipeline warm up, this runs enough training to produce the first set of gradients.\n",
    "            gpu_grad_stage_ops.append(grad_stage_op)\n",
    "            grads = grad_stage.get()\n",
    "            \n",
    "        param_refs = self.variable_mgr.trainable_variables_on_device(\n",
    "            device_num, writable = True)\n",
    "        gradvars = zip(grads, param_refs)\n",
    "        return (loss, gradvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_sync_queues_and_barrier(self, name_prefix,\n",
    "                                   enqueue_after_list):\n",
    "    '''Add ops to enqueue on all worker queues'''\n",
    "    # name_prefix: prefixed for the shared name of ops.\n",
    "    # enqueue_after_list: control dependency from ops\n",
    "    # Returns: an op that should be used as control dependency before starting next step. (This op would only \n",
    "    # be executed on parameter devices)\n",
    "    self.sync_queue_counter += 1\n",
    "    num_workers = self.cluster.num_tasks('worker')\n",
    "    with tf.device(self.sync_queue_devices[\n",
    "            self.sync_queue_counter % len(self.sync_queue_devices)\n",
    "    ]):\n",
    "        sync_queues = [tf.FIFOQueue(num_workers, [tf.bool], shape = [[]], shared_name = '%s%s' % (name_prefix, i))\n",
    "         for i in range(num_workers)]\n",
    "        queue_ops = []\n",
    "        # For each worker, add an entry in a queue, signaling that it can finish this step.\n",
    "        token = tf.constant(False)\n",
    "        with tf.control_dependencies(enqueue_after_list):\n",
    "            ##About tf.control_dependencies here:\n",
    "            # Returns a context manager that specifies control dependencies. \n",
    "            # Use with the 'with' keyword to specify that all operations constructed within the context should\n",
    "            # have control dependencies on 'control_inputs'.\n",
    "            # For example:\n",
    "            # '''python\n",
    "            # with g.control_dependencies([a, b, c]):\n",
    "            #   #'d' and 'e' will only run after 'a', 'b', and 'c' have executed\n",
    "            # d = ..., e = ...\n",
    "            #'''\n",
    "            # Multiple calls to 'control_dependencies()' can be nested, and in that case a new 'Op' will have control\n",
    "            # dependencies on the union of 'control_inputs' from all active contexts.\n",
    "            # Also you can pass None to clear the control dependencies.\n",
    "            # Also, the control dependencies context applies *only* to ops that are constructed within the context.\n",
    "            # Merely using an op or tensor in the context does not add a control dependencies.\n",
    "            for i, q in enumerate(sync_queues):\n",
    "                if i == self.task_index:\n",
    "                    queue_ops.append(tf.no_op())\n",
    "                else:\n",
    "                    queue_ops.append(q.enqueue(token))\n",
    "            \n",
    "        #Drain tokens off queue for this worker, one for each other worker.\n",
    "        queue_ops.append(sync_queues[self.task_index].dequeue_many(len(sync_queues) - 1))\n",
    "        \n",
    "    return tf.group(*queue_ops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
