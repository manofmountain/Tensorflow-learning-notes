{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is from 'MNIST For ML Beginners' of tensorflow learning guide. And it's an explanation tutorial, line by line of what's happending in the mnist_softmax.py code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this tutorial we can learn below contents:\n",
    "* Learn about the MNIST data and softmax regressions;\n",
    "* Create a function that's model for recognizing digits, based on looking every pixel in the image;\n",
    "* Use tensorflow to train the model to recognize digits by having it \"look\" at thousands of examples;\n",
    "* Check the model's accuracy with our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is hosted on Yann LeCun's websit(yann.lecun.com/exdb/mnist/). We can download and read in the data automatically with below two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is split into three parts: 55000 data points of training data; 10000 data points of test data and 5000 data points of validation data. \n",
    "It's essential in machine learning that we have seperate data which we don't learn from so that we can make sure that what we've learned actually generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x65cc590>\n",
      "55000\n",
      "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x65cca90>\n",
      "10000\n",
      "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x65ccb50>\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "#Check 'mnist' datasets\n",
    "print mnist.train\n",
    "print mnist.train.num_examples\n",
    "print mnist.test\n",
    "print mnist.test.num_examples\n",
    "print mnist.validation\n",
    "print mnist.validation.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do efficient numerical computing in Python, we typically use libraries like NumPy that do expensive operations such as matrix multiplication outside Python, using hightly efficient code implemented in another language. Unfortunately there can still be a lot of overhead from switching back to Python every operation.\n",
    "Tensorflow also does its heavy lifting outside Python, but it takes things a step further to avoid this overhead. Instead of running a single expensive operation independently from Python,, tensorflow lets us describe a graph of interacting operations that run entirely outside Python.\n",
    "\n",
    "Below are graph building procedure.\n",
    "Like other framework as Theano or Torch, tensorflow lets us describe a graph of interacting operations that run entirely outside Python. So we use python ( actually a wrapper ) only to construct the graph, then run it with C++ (the more efficient language ) as the backend. The role of the Python code is therefore to build this external computation graph, and to dictate which parts of the computation graph should be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', <tf.Tensor 'Placeholder_1:0' shape=(?, 784) dtype=float32>)\n",
      "('W', <tensorflow.python.ops.variables.Variable object at 0x65de610>)\n",
      "('b', <tensorflow.python.ops.variables.Variable object at 0x65de850>)\n",
      "('y', <tf.Tensor 'Softmax_1:0' shape=(?, 10) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#x is a 'placeholder', a value that'll input when we ask tensorflow to run a computation. We want to be able to input any \n",
    "#number of MNIST images, each flattened into a 784-dimensional vector. We represent this as a 2-D tensor of floating point\n",
    "#numbers with a shape [None, 784]. (Here None means a dimension can be of any length.)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "print('x', x)\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "print('W', W)\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "print('b', b)\n",
    "#Below one line ddefines our final model\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "print('y', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use cross-entropy to evaluate if our model is good or bad.\n",
    "To implement cross-entropy we need to add a new placeholder to input the correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "#then we implement cross-entropy function\n",
    "#cross_entroy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices = [1]))\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y, labels = y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the whole graph, we will start to train and learn its parameters. All the work should be done in a session.\n",
    "Tensorflow relies a highly efficient C++ backend to do its computation. The connection to this backend is called a session. The common usuage for tensorflow programs is to first create a graph and then launch it in a session.\n",
    "Here we instead use the convenient InteractiveSession class, which makes tensorflow more flexible about how you structure your code. It allows you to interleave operations which build a computation graph with ones that run the graph.\n",
    "This is particularly convenient when working in interactive contexts like IPython. If you're not using an InteractiveSession, then you should build the entire computation graph before starting a session and lanuching the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready, we had the graph, had the session. Then we can start the training iteration like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('W', array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32))\n",
      "('b', array([-0.22346789,  0.37578991, -0.03983562, -0.25641957,  0.18791199,\n",
      "        0.95993721,  0.04604222,  0.51998931, -1.30617368, -0.26377633], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "for _ in xrange(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    #or use below codes instead\n",
    "    #batch = mnist.train.next_batch(100)\n",
    "    #train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "print('W', sess.run(W))\n",
    "print('b', sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using small batches of random data is called stochastic training -- in this case, stochastic gradient descent. Ideally, we'd like to use all our data for every step of training because that would give us a better sense of what we would be doing, but that's expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we need to know if our trained model can predict the correct label. tf.argmax is an extreamely useful function which gives us the index of the highest entry in a tensor along some axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9183\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "#or use below codes\n",
    "#print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
