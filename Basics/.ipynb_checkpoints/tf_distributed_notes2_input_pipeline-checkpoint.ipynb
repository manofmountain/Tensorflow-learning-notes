{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tensorflow, we need to build a fully input pipline to fully utilize multi-cores or multi-nodes' execution abilities. There're native parallelism existed within tensorflow. The implementation is made up of 3 stages:\n",
    "* I/O reads: Choose and read image files from disk;\n",
    "* Image processing: Decode image records into images, preprocess, and organized into mini-batches;\n",
    "* CPU-to-GPU Data Transfer: Transfer images from CPU to GPU.\n",
    "\n",
    "The dominant part of each stage is executed in parallel with the other stages using data_flow_ops.StagingArea.\n",
    "StagingArea is a queue-like operator similar to tf.FIFOQueue. The difference is that StagingArea offers simpler functionality and can be executed on both CPU and GPU in parallel with other stages.\n",
    "\n",
    "Breaking the input pipeline into 3 stages that operate independently in parallel is scallable and takes full advantage of large multi-core environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2. Parallelize I/O Reads\n",
    "\n",
    "dataflow_ops.RecordInput is used to parallelize reading from disk. Given a list of input files representing TFRecords, RecordInput continuously reads records using background threads. The records are placed into its own large internal pool and when it has loaded at least half of its capacity, it produces output tensors.\n",
    "\n",
    "This op has its own internal threads that are dominated by I/O time that consumes minimal CPU, which allows it to run smoothly in parallel with the rest of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "\n",
    "record_input = data_flow_ops.RecordInput(\n",
    "        file_pattern = dataset.tf_record_pattern(subset),\n",
    "        seed = 301,\n",
    "        parallelism = 64,\n",
    "        buffer_size = 10000,\n",
    "        batch_size = 256,\n",
    "        name = 'record_input')\n",
    "records = record_input.get_yield_op()\n",
    "records = tf.split(records, self.batch_size, 0)\n",
    "records = [tf.reshape(record, []) or record in records]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can use a wrapper class 'RecordInput' provided by data_flow_ops to input data parallelly. And its interface can be seen like below:\n",
    "class RecordInput(object):\n",
    "    '''RecordInput asynchronously reads and randomly yields TFRecords.\n",
    "    A RecordInput Op will continuously read a batch of records asynchronously into a buffer of some fixed capacity.\n",
    "    It can also asynchronously yield random records from this buffer.\n",
    "    It will not start yielding until at least 'buffer_size / 2' elements have been placed into the buffer so that\n",
    "    sufficient randomization can take place.\n",
    "    \n",
    "    The order the files are read will be shifted each epoch by 'shift_amount' so that the data is presented in a \n",
    "    different order every epoch.'''\n",
    "    def __init__(self,\n",
    "                file_pattern,\n",
    "                batch_size = 1,\n",
    "                buffer_size = 1,\n",
    "                parallelism = 1,\n",
    "                shift_ratio = 0,\n",
    "                seed = 0,\n",
    "                name = None):\n",
    "     '''Constructs a RecordInput Op.\n",
    "     \n",
    "     Args:\n",
    "         file_pattern: File path to the dataset, possibly containing wildcards.All matching files will be iterated\n",
    "         over each epoch;\n",
    "         batch_size: How many records to return at a time;\n",
    "         buffer_size: The maximum number of records the buffer will contain. This must be smaller than the total \n",
    "         number of records in an epoch or deadlock can occur;\n",
    "         parallelism: How many reader threads to use for reading from files;\n",
    "         shift_ratio: What percentage of the total number files to move the start file forward by each epoch;\n",
    "         seed: Specify the random number seed used by generator that randomizes records;\n",
    "         name: Optional name for the operation.\n",
    "    Raises:\n",
    "        ValueError: If one of the arguments is invalid.\n",
    "     '''\n",
    "Then in its member function 'get_yield_op', it will call the function of gen_data_flow_ops module to yield a minibatch every time it is executed.\n",
    "\n",
    "We walked through code within gen_data_flow_ops and the definition of 'record_input' can be seen like below:\n",
    "    def record_input(file_pattern, file_random_seed = None, file_shuffle_shift_ratio=None, file_buffer_size = None,\n",
    "                    file_parallelism = None, batch_size = None, name = None):\n",
    "        r'''Emits randomized records.\n",
    "        \n",
    "        Args:\n",
    "            file_pattern: A 'string'. Glob pattern for the data files;\n",
    "            file_randome_seed: An optional 'int'. Defaults to '301'.Random seeds used to produce randomized records;\n",
    "            file_shuffle_shift_ratio: An optional 'float'. Defaults to '0'. Shifts the list of files after the list \n",
    "            is randomly shuffled;\n",
    "            file_buffer_size: An optional 'int'. Defaults to '10000'. The randomization shuffling buffer;\n",
    "            file_parallelism: An optional 'int'. Defaults to '16'. How many sstables are opened and concurrently \n",
    "            iterated over.\n",
    "            batch_size: An optional 'int'. Defaults to '32'. The batch size;\n",
    "            name: A name for the operation (optional).\n",
    "       Returns:\n",
    "           A 'Tensor' of type 'string'. A tensor of shape [batch_size].\n",
    "        '''\n",
    "        ##The result will be obtained through native ops provided by c++ compiled library\n",
    "        result = _op_def_lib.apply_op(\"RecordInput\",\n",
    "                file_pattern = file_pattern,\n",
    "                file_random_seed = file_random_seed,\n",
    "                file_shuffle_shift_ratio = file_shuffle_shift_ratio,\n",
    "                file_buffer_size = file_buffer_size,\n",
    "                file_parallelism = file_parallelism,\n",
    "                batch_size = batch_size,\n",
    "                name = name)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2. Parallelize Image Processing\n",
    "\n",
    "After images are read from RecordInput they are passed as tensors to the image processing pipeline.\n",
    "Here we assume that the input pipeline is targeting 8 GPUs with a batch size of 256 ( 32 per GPU ).\n",
    "\n",
    "256 records are read and processed individually in parallel. This starts with 256 independent RecordInput read ops in the graph. Each read op is followed by an identical set of ops for image preprocessing that are considered independent and executed in parallel. The image preprocessing ops include operations such as image decoding, distortion and resizing.\n",
    "\n",
    "Once the images are through preprocessing, they are concatenated together into 8 batch size 32 tensors. Rather than\n",
    "using tf.concat for this purpose, which is implemented as a single op that waits for all the inputs to be ready before concatenating them together, tf.parallel_stack is used. tf.parallel_stack allocates an uninitialized tensor as an output, and each input tensor is written to its designated portion of the output tensor as soon as the input is available.\n",
    "\n",
    "When all the input tensors are finished, the output tensor is passed along in the graph. This effectively hides all the memory latency with the long tail of producing all the input tensors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
